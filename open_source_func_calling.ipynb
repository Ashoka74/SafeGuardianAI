{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# test this :\n",
    "# https://pyjnius.readthedocs.io/en/latest/android.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 27 key-value pairs and 219 tensors from C:/Users/sinan/OneDrive/Desktop/projects/SafeGuardian/EQ_Rescue_System/xLAM-1b-fc-r.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = xlam-FC-1b\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5504\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 100000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                    llama.rope.scaling.type str              = linear\n",
      "llama_model_loader: - kv  14:                  llama.rope.scaling.factor f32              = 4.000000\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = deepseek-coder\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,32256]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,31757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 32013\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 32021\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 32014\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% set system_message = 'You are an A...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type q5_0:   12 tensors\n",
      "llama_model_loader: - type q8_0:   12 tensors\n",
      "llama_model_loader: - type q4_K:  133 tensors\n",
      "llama_model_loader: - type q6_K:   13 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.1787 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 32256\n",
      "llm_load_print_meta: n_merges         = 31757\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 5504\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 100000.0\n",
      "llm_load_print_meta: freq_scale_train = 0.25\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 1.35 B\n",
      "llm_load_print_meta: model size       = 831.88 MiB (5.18 BPW) \n",
      "llm_load_print_meta: general.name     = xlam-FC-1b\n",
      "llm_load_print_meta: BOS token        = 32013 '<｜begin▁of▁sentence｜>'\n",
      "llm_load_print_meta: EOS token        = 32021 '<|EOT|>'\n",
      "llm_load_print_meta: PAD token        = 32014 '<｜end▁of▁sentence｜>'\n",
      "llm_load_print_meta: LF token         = 126 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =   831.88 MiB\n",
      "....................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 100000.0\n",
      "llama_new_context_with_model: freq_scale = 0.25\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    67.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'xlam-FC-1b', 'general.architecture': 'llama', 'llama.block_count': '24', 'llama.context_length': '16384', 'tokenizer.ggml.eos_token_id': '32021', 'general.file_type': '15', 'llama.attention.head_count_kv': '16', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '5504', 'llama.attention.head_count': '16', 'llama.rope.freq_base': '100000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.vocab_size': '32256', 'llama.rope.dimension_count': '128', 'llama.rope.scaling.type': 'linear', 'llama.rope.scaling.factor': '4.000000', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'deepseek-coder', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '32013', 'tokenizer.ggml.padding_token_id': '32014', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{% set system_message = 'You are an AI assistant for function calling.For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\\\n' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '### Instruction:\\\\n' + content + '\\\\n### Response:' }}{% elif message['role'] == 'assistant' %}{{ '\\\\n' + content + '\\\\n<|EOT|>\\\\n' }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set system_message = 'You are an AI assistant for function calling.For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\n' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '### Instruction:\\n' + content + '\\n### Response:' }}{% elif message['role'] == 'assistant' %}{{ '\\n' + content + '\\n<|EOT|>\\n' }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|EOT|>\n",
      "Using chat bos_token: <｜begin▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "      model_path=\"C:/Users/sinan/OneDrive/Desktop/projects/SafeGuardian/EQ_Rescue_System/xLAM-1b-fc-r.Q4_K_M.gguf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''You are an AI assistant for function calling.\n",
    "### Instruction:\n",
    "[BEGIN OF TASK INSTRUCTION]\n",
    "{task_instruction}\n",
    "[END OF TASK INSTRUCTION]\n",
    "\n",
    "[BEGIN OF AVAILABLE TOOLS]\n",
    "{xlam_format_tools}\n",
    "[END OF AVAILABLE TOOLS]\n",
    "\n",
    "[BEGIN OF FORMAT INSTRUCTION]\n",
    "{format_instruction}\n",
    "[END OF FORMAT INSTRUCTION]\n",
    "\n",
    "[BEGIN OF QUERY]\n",
    "{query}\n",
    "[END OF QUERY]\n",
    "\n",
    "### Response:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m output \u001b[38;5;241m=\u001b[39m llm(\n\u001b[0;32m      4\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an AI assistant for function calling.For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m### Instruction:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[BEGIN OF TASK INSTRUCTION]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou are an expert in composing functions. You are given a question and a set of possible functions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBased on the question, you will need to make one or more function/tool calls to achieve the purpose.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf none of the functions can be used, point it out and refuse to answer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf the given question lacks the parameters required by the function, also point it out.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[END OF TASK INSTRUCTION]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[BEGIN OF AVAILABLE TOOLS]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m{\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mname\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mget_weather\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mGet the current weather for a location\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mThe city and state, e.g. San Francisco, CA\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m}, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124munit\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124menum\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: [\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mcelsius\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mfahrenheit\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m], \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mThe unit of temperature to return\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m}}}\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[END OF AVAILABLE TOOLS]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[BEGIN OF FORMAT INSTRUCTION]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe output MUST strictly adhere to the following JSON format, and NO other text MUST be included.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe example format is as follows. Please make sure the parameter type is correct. If no function call is needed, please make tool_calls an empty list \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m{\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: [\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mname\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mfunc_name1\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124marguments\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124margument1\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mvalue1\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124margument2\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mvalue2\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m}},\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    ... (more tool calls as required)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  ]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m}\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[END OF FORMAT INSTRUCTION]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[BEGIN OF QUERY]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[END OF QUERY]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m### Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m       echo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# Echo the prompt back in the output\u001b[39;00m\n\u001b[0;32m      6\u001b[0m ) \u001b[38;5;66;03m# Generate a completion, can also call create_completion\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "input = input()\n",
    "\n",
    "output = llm(\n",
    "      \"You are an AI assistant for function calling.For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\n### Instruction:\\n[BEGIN OF TASK INSTRUCTION]\\nYou are an expert in composing functions. You are given a question and a set of possible functions.\\nBased on the question, you will need to make one or more function/tool calls to achieve the purpose.\\nIf none of the functions can be used, point it out and refuse to answer.\\nIf the given question lacks the parameters required by the function, also point it out.\\n[END OF TASK INSTRUCTION]\\n\\n[BEGIN OF AVAILABLE TOOLS]\\n{\\\"name\\\": \\\"get_weather\\\", \\\"description\\\": \\\"Get the current weather for a location\\\", \\\"parameters\\\": {\\\"location\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\"}, \\\"unit\\\": {\\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The unit of temperature to return\\\"}}}\\n[END OF AVAILABLE TOOLS]\\n\\n[BEGIN OF FORMAT INSTRUCTION]\\nThe output MUST strictly adhere to the following JSON format, and NO other text MUST be included.\\nThe example format is as follows. Please make sure the parameter type is correct. If no function call is needed, please make tool_calls an empty list '[]'\\n```\\n{\\n  \\\"tool_calls\\\": [\\n    {\\\"name\\\": \\\"func_name1\\\", \\\"arguments\\\": {\\\"argument1\\\": \\\"value1\\\", \\\"argument2\\\": \\\"value2\\\"}},\\n    ... (more tool calls as required)\\n  ]\\n}\\n```\\n[END OF FORMAT INSTRUCTION]\\n\\n[BEGIN OF QUERY]\\n\" + input + \"\\n[END OF QUERY]\\n\\n\\n### Response:\",\n",
    "      echo=False # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\"tool_calls\": [{\"name\": \"get_weather\",'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
